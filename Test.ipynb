{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_weights(model, dst_fn):\n",
    "    weights = model.get_weights()\n",
    "    with open(dst_fn, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "\n",
    "def load_weights(model, weight_fn):\n",
    "    with open(weight_fn, 'rb') as f:\n",
    "        weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    return model\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    # adjust the targets for sequence bucketing\n",
    "    ll = tf.shape(y_pred)[1]\n",
    "    y_true = y_true[:, :ll]\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n",
    "        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n",
    "\n",
    "    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n",
    "    max_len = tf.reduce_max(lens)\n",
    "    ids_ = ids[:, :max_len]\n",
    "    att_ = att[:, :max_len]\n",
    "    tok_ = tok[:, :max_len]\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n",
    "    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "    \n",
    "    # this is required as `model.predict` needs a fixed size!\n",
    "    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n",
    "    \n",
    "    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n",
    "    return model, padded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "preds_start1 = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end1 = np.zeros((input_ids_t.shape[0],MAX_LEN)) \n",
    "for i in range(5):\n",
    "    K.clear_session()\n",
    "    model, padded_model = build_model()\n",
    "    print('load model...')\n",
    "    weight_fn = '/kaggle/input/with-processing/v0-roberta-%i.h5'%(i)\n",
    "    load_weights(model, weight_fn)\n",
    "\n",
    "    print('Predicting Test...')\n",
    "    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n",
    "    preds_start1 += preds[0]/n_splits\n",
    "    preds_end1 += preds[1]/n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start1[k,])\n",
    "    b = np.argmax(preds_end1[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-2:b-1])\n",
    "    all.append(st)\n",
    "test['selected_text1'] = all\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_preprocessing(text):\n",
    "\n",
    "    text = text.replace(\". . . .\", \"....\")\n",
    "    text = text.replace(\". . .\", \"...\")\n",
    "    text = text.replace(\". .\", \"..\")\n",
    "    text = text.replace(\"! ! ! !\", \"!!!!\")\n",
    "    text = text.replace(\"! ! !\", \"!!!\")\n",
    "    text = text.replace(\"! !\", \"!!\")\n",
    "    text = text.replace(\"? ? ? ?\", \"????\")\n",
    "    text = text.replace(\"? ? ?\", \"???\")\n",
    "    text = text.replace(\"? ?\", \"??\")\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def find_text_idx(text, selected_text):\n",
    "\n",
    "    text_len = len(text)\n",
    "\n",
    "    for start_idx in range(text_len):\n",
    "        if text[start_idx] == selected_text[0]:\n",
    "            for end_idx in range(start_idx+1, text_len+1):\n",
    "                contained_text = \"\".join(text[start_idx: end_idx].split())\n",
    "                # print(\"contained_text:\", contained_text, \"selected_text:\", selected_text)\n",
    "                if contained_text == \"\".join(selected_text.split()):\n",
    "                    return start_idx, end_idx\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def calculate_spaces(text, selected_text):\n",
    "\n",
    "    selected_text = \" \".join(selected_text.split())\n",
    "    start_idx, end_idx = find_text_idx(text, selected_text)\n",
    "    # print(\"text:\", text[start_idx: end_idx], \"prediction:\", selected_text)\n",
    "\n",
    "    if start_idx is None:\n",
    "        start_idx = 0\n",
    "        print(\"----------------- error no start idx find ------------------\")\n",
    "        print(\"text:\", text, \"prediction:\", selected_text)\n",
    "        print(\"----------------- error no start idx find ------------------\")\n",
    "\n",
    "    if end_idx is None:\n",
    "        end_idx = len(text)\n",
    "        print(\"----------------- error no end idx find ------------------\")\n",
    "        print(\"text:\", text, \"prediction:\", selected_text)\n",
    "        print(\"----------------- error no end idx find ------------------\")\n",
    "\n",
    "    x = text[:start_idx]\n",
    "    try:\n",
    "        if x[-1] == \" \":\n",
    "            x = x[:-1]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    l1 = len(x)\n",
    "    l2 = len(\" \".join(x.split()))\n",
    "    return l1 - l2, start_idx, end_idx\n",
    "\n",
    "\n",
    "def pp_v2(text, predicted):\n",
    "\n",
    "    text = str(text).lower()\n",
    "    predicted = predicted.lower()\n",
    "    predicted = predicted.strip()\n",
    "\n",
    "    if len(predicted) == 0:\n",
    "        return predicted\n",
    "\n",
    "    predicted = reverse_preprocessing(str(predicted))\n",
    "\n",
    "    spaces, index_start, index_end = calculate_spaces(text, predicted)\n",
    "\n",
    "    if spaces == 1:\n",
    "        if len(text[max(0, index_start-1): index_end+1]) <= 0 or text[max(0, index_start-1): index_end+1][-1] != \".\":\n",
    "            return text[max(0, index_start - 1): index_end]\n",
    "        else:\n",
    "            return text[max(0, index_start-1): index_end+1]\n",
    "    elif spaces == 2:\n",
    "        return text[max(0, index_start-2): index_end]\n",
    "    elif spaces == 3:\n",
    "        return text[max(0, index_start-3): index_end-1]\n",
    "    elif spaces == 4:\n",
    "        return text[max(0, index_start-4): index_end-2]\n",
    "    else:\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"selected_text\"] = test.apply(lambda x: pp_v2(x.text, x.selected_text1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}